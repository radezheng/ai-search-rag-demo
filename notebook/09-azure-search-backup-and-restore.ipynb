{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search backup and restore sample\n",
    "\n",
    "This notebook demonstrates how to backup and restore a search index and migrate it to another instance.\n",
    "\n",
    "The only pre-requsitite is that your search index has a `key` field that is `filterable` and `sortable`. If you don't have one, you can create a new field and assign unique values to your search index. \n",
    "\n",
    "It is important to note that only fields marked as `retrievable` can be successfully backed up and restored. It's crucial to consider whether or not you want your vector fields to be marked as `retrievable` in your search index. Marking vector fields as `retrievable` will allow you to backup and restore them and use them for any purpose, whereas NOT marking them as `retrievable` will save you storage costs, but the tradeoff is that you will not be able to backup and restore those fields.\n",
    "\n",
    "Please review this sample and follow the instructions provided in this Jupyter Python notebook to backup and restore your Azure AI Search indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-search-documents\n",
      "  Downloading azure_search_documents-11.4.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting azure-core<2.0.0,>=1.28.0 (from azure-search-documents)\n",
      "  Using cached azure_core-1.29.6-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting azure-common~=1.1 (from azure-search-documents)\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting isodate>=0.6.0 (from azure-search-documents)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\zhzhen\\appdata\\roaming\\python\\python310\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-search-documents) (1.16.0)\n",
      "Collecting typing-extensions>=4.6.0 (from azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading azure_search_documents-11.4.0-py3-none-any.whl (283 kB)\n",
      "   ---------------------------------------- 0.0/283.8 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/283.8 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/283.8 kB 487.6 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 81.9/283.8 kB 657.6 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/283.8 kB 853.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/283.8 kB 908.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 245.8/283.8 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 283.8/283.8 kB 1.0 MB/s eta 0:00:00\n",
      "Using cached azure_core-1.29.6-py3-none-any.whl (192 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: azure-common, urllib3, typing-extensions, sniffio, isodate, idna, exceptiongroup, charset-normalizer, certifi, requests, anyio, azure-core, azure-search-documents\n",
      "Successfully installed anyio-4.2.0 azure-common-1.1.28 azure-core-1.29.6 azure-search-documents-11.4.0 certifi-2023.11.17 charset-normalizer-3.3.2 exceptiongroup-1.2.0 idna-3.6 isodate-0.6.1 requests-2.31.0 sniffio-1.3.0 typing-extensions-4.9.0 urllib3-2.1.0\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhzhen\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script demonstrates backing up and restoring an Azure AI Search index between two services. The `backup_and_restore_index` function retrieves the source index definition, creates a new target index, backs up all documents, and restores them to the target index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The key field is not filterable or not sortable. A maximum of 100,000 records can be backed up and restored.\n",
      "Backing up and restoring documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:05<00:00, 21.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents uploaded successfully.\n",
      "Successfully backed up 'gptkbindex2' and restored to 'gptkbindex2-new'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "import tqdm  \n",
    "import os\n",
    "\n",
    "def create_clients(endpoint, key, index_name):  \n",
    "    search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(key))  \n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(key))  \n",
    "    return search_client, index_client\n",
    "\n",
    "def total_count(search_client):\n",
    "    response = search_client.search(include_total_count=True, search_text=\"*\", top=0)\n",
    "    return response.get_count()\n",
    "  \n",
    "def search_results_with_filter(search_client, key_field_name):\n",
    "    last_item = None\n",
    "    response = search_client.search(search_text=\"*\", top=100000, order_by=key_field_name).by_page()\n",
    "    while True:\n",
    "        for page in response:\n",
    "            page = list(page)\n",
    "            if len(page) > 0:\n",
    "                last_item = page[-1]\n",
    "                yield page\n",
    "            else:\n",
    "                last_item = None\n",
    "        \n",
    "        if last_item:\n",
    "            response = search_client.search(search_text=\"*\", top=100000, order_by=key_field_name, filter=f\"{key_field_name} gt '{last_item[key_field_name]}'\").by_page()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def search_results_without_filter(search_client):\n",
    "    response = search_client.search(search_text=\"*\", top=100000).by_page()\n",
    "    for page in response:\n",
    "        page = list(page)\n",
    "        yield page\n",
    "\n",
    "def backup_and_restore_index(source_endpoint, source_key, source_index_name, target_endpoint, target_key, target_index_name):  \n",
    "    # Create search and index clients  \n",
    "    source_search_client, source_index_client = create_clients(source_endpoint, source_key, source_index_name)  \n",
    "    target_search_client, target_index_client = create_clients(target_endpoint, target_key, target_index_name)  \n",
    "  \n",
    "    # Get the source index definition  \n",
    "    source_index = source_index_client.get_index(name=source_index_name)\n",
    "    non_retrievable_fields = []\n",
    "    for field in source_index.fields:\n",
    "        if field.hidden == True:\n",
    "            non_retrievable_fields.append(field)\n",
    "        if field.key == True:\n",
    "            key_field = field\n",
    "\n",
    "    if not key_field:\n",
    "        raise Exception(\"Key Field Not Found\")\n",
    "    \n",
    "    if len(non_retrievable_fields) > 0:\n",
    "        print(f\"WARNING: The following fields are not marked as retrievable and cannot be backed up and restored: {', '.join(f.name for f in non_retrievable_fields)}\")\n",
    "  \n",
    "    # Create target index with the same definition \n",
    "    source_index.name = target_index_name\n",
    "    target_index_client.create_or_update_index(source_index)\n",
    "  \n",
    "    document_count = total_count(source_search_client)\n",
    "    can_use_filter = key_field.sortable and key_field.filterable\n",
    "    if not can_use_filter:\n",
    "        print(\"WARNING: The key field is not filterable or not sortable. A maximum of 100,000 records can be backed up and restored.\")\n",
    "    # Backup and restore documents  \n",
    "    all_documents = search_results_with_filter(source_search_client, key_field.name) if can_use_filter else search_results_without_filter(source_search_client)\n",
    "\n",
    "    print(\"Backing up and restoring documents:\")  \n",
    "    failed_documents = 0  \n",
    "    failed_keys = []  \n",
    "    with tqdm.tqdm(total=document_count) as progress_bar:  \n",
    "        for page in all_documents:\n",
    "            result = target_search_client.upload_documents(documents=page)  \n",
    "            progress_bar.update(len(result))  \n",
    "  \n",
    "            for item in result:  \n",
    "                if item.succeeded is not True:  \n",
    "                    failed_documents += 1\n",
    "                    failed_keys.append(page[result.index_of(item)].id)  \n",
    "                    print(f\"Document upload error: {item.error.message}\")  \n",
    "  \n",
    "    if failed_documents > 0:  \n",
    "        print(f\"Failed documents: {failed_documents}\")  \n",
    "        print(f\"Failed document keys: {failed_keys}\")  \n",
    "    else:  \n",
    "        print(\"All documents uploaded successfully.\")  \n",
    "  \n",
    "    print(f\"Successfully backed up '{source_index_name}' and restored to '{target_index_name}'\")  \n",
    "    return source_search_client, target_search_client, all_documents  \n",
    "  \n",
    "# Replace with your service endpoints, keys, and index names\n",
    "\n",
    "\n",
    "source_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "source_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "source_index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "target_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "target_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "target_index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") + \"-new\"\n",
    "\n",
    "source_search_client, target_search_client, all_documents = backup_and_restore_index(source_endpoint, source_key, source_index_name, target_endpoint, target_key, target_index_name)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verify_counts function compares document counts between source and target indexes after backup and restore. It prints a message indicating if the document counts match or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source document count: 123\n",
      "Target document count: 123\n",
      "Document counts match.\n"
     ]
    }
   ],
   "source": [
    "def verify_counts(source_search_client, target_search_client):  \n",
    "    source_document_count = source_search_client.get_document_count()  \n",
    "    target_document_count = target_search_client.get_document_count()  \n",
    "  \n",
    "    print(f\"Source document count: {source_document_count}\")  \n",
    "    print(f\"Target document count: {target_document_count}\")  \n",
    "  \n",
    "    if source_document_count == target_document_count:  \n",
    "        print(\"Document counts match.\")  \n",
    "    else:  \n",
    "        print(\"Document counts do not match.\")  \n",
    "  \n",
    "# Call the verify_counts function with the search_clients returned by the backup_and_restore_index function  \n",
    "verify_counts(source_search_client, target_search_client)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
